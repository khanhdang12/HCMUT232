{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ixeCENBNh9Rd",
        "outputId": "f2456049-db8c-4530-873c-2c07161a8d1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyvi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G8vMOkISn8xb",
        "outputId": "313aca80-3798-45c9-88c4-60860cdb90cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyvi\n",
            "  Downloading pyvi-0.1.1-py2.py3-none-any.whl (8.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from pyvi) (1.2.2)\n",
            "Collecting sklearn-crfsuite (from pyvi)\n",
            "  Downloading sklearn_crfsuite-0.3.6-py2.py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->pyvi) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->pyvi) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->pyvi) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->pyvi) (3.5.0)\n",
            "Collecting python-crfsuite>=0.8.3 (from sklearn-crfsuite->pyvi)\n",
            "  Downloading python_crfsuite-0.9.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m58.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from sklearn-crfsuite->pyvi) (1.16.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from sklearn-crfsuite->pyvi) (0.9.0)\n",
            "Requirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.10/dist-packages (from sklearn-crfsuite->pyvi) (4.66.4)\n",
            "Installing collected packages: python-crfsuite, sklearn-crfsuite, pyvi\n",
            "Successfully installed python-crfsuite-0.9.10 pyvi-0.1.1 sklearn-crfsuite-0.3.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from string import digits\n",
        "from collections import Counter\n",
        "from pyvi import ViTokenizer\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "# from keras.utils.np_utils import to_categorical\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "Af1iFQKRoF3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_train = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/NLP/Lab6/vlsp_sentiment_train.csv\", sep='\\t')\n",
        "data_train.columns =['Class', 'Data']\n",
        "data_test = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/NLP/Lab6/vlsp_sentiment_test.csv\", sep='\\t')\n",
        "data_test.columns =['Class', 'Data']"
      ],
      "metadata": {
        "id": "1xrbp-e9oshJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data_train.shape)\n",
        "print(data_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "28HQLnnnpEdB",
        "outputId": "178e668f-121b-494e-8f3a-d6f79f6c0d31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5100, 2)\n",
            "(1050, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels = data_train.iloc[:, 0].values\n",
        "reviews = data_train.iloc[:, 1].values"
      ],
      "metadata": {
        "id": "v_X-ZSJRpGSe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_labels = []\n",
        "\n",
        "for label in labels:\n",
        "    if label == -1:\n",
        "        encoded_labels.append([1,0,0])\n",
        "    elif label == 0:\n",
        "        encoded_labels.append([0,1,0])\n",
        "    else:\n",
        "        encoded_labels.append([0,0,1])\n",
        "\n",
        "encoded_labels = np.array(encoded_labels)"
      ],
      "metadata": {
        "id": "eS7icHdLpIcQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reviews_processed = []\n",
        "unlabeled_processed = []\n",
        "for review in reviews:\n",
        "    review_cool_one = ''.join([char for char in review if char not in digits])\n",
        "    reviews_processed.append(review_cool_one)"
      ],
      "metadata": {
        "id": "9pdd94lhpKqz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Use PyVi for Vietnamese word tokenizer\n",
        "word_reviews = []\n",
        "all_words = []\n",
        "for review in reviews_processed:\n",
        "    review = ViTokenizer.tokenize(review.lower())\n",
        "    word_reviews.append(review.split())"
      ],
      "metadata": {
        "id": "hRIJqf_dpfZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EMBEDDING_DIM = 400 # how big is each word vector\n",
        "MAX_VOCAB_SIZE = 10000 # how many unique words to use (i.e num rows in embedding vector)\n",
        "MAX_SEQUENCE_LENGTH = 300 # max number of words in a comment to use"
      ],
      "metadata": {
        "id": "_ajDQ7ctph6o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "metadata": {
        "id": "LecBKgIvpjlu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE, lower=True, char_level=False)\n",
        "tokenizer.fit_on_texts(word_reviews)\n",
        "sequences_train = tokenizer.texts_to_sequences(word_reviews)\n",
        "word_index = tokenizer.word_index"
      ],
      "metadata": {
        "id": "YGDA2LMOplPW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pad_sequences(sequences_train, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "labels = encoded_labels"
      ],
      "metadata": {
        "id": "Wu6TB4hCpnZz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Shape of X train and X validation tensor:',data.shape)\n",
        "print('Shape of label train and validation tensor:', labels.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D_ay9-aupoxX",
        "outputId": "37c3d876-2aee-479b-bd6b-58a11f38a757"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X train and X validation tensor: (5100, 300)\n",
            "Shape of label train and validation tensor: (5100, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.utils import simple_preprocess\n",
        "\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "\n",
        "word_vectors = KeyedVectors.load_word2vec_format('/content/drive/MyDrive/Colab Notebooks/NLP/Lab5/vi-model-CBOW.bin', binary=True)\n",
        "\n",
        "vocabulary_size=min(len(word_index)+1,MAX_VOCAB_SIZE)\n",
        "embedding_matrix = np.zeros((vocabulary_size, EMBEDDING_DIM))\n",
        "print(\"vocab size\", vocabulary_size)\n",
        "for word, i in word_index.items():\n",
        "    if i>=MAX_VOCAB_SIZE:\n",
        "        continue\n",
        "    try:\n",
        "        embedding_vector = word_vectors[word]\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "    except KeyError:\n",
        "        embedding_matrix[i]=np.random.normal(0,np.sqrt(0.25),EMBEDDING_DIM)\n",
        "\n",
        "del(word_vectors)\n",
        "\n",
        "from keras.layers import Embedding\n",
        "embedding_layer = Embedding(vocabulary_size,\n",
        "                            EMBEDDING_DIM,\n",
        "                            weights=[embedding_matrix],\n",
        "                            trainable=True)"
      ],
      "metadata": {
        "id": "77ykdd5Sp1Jr",
        "outputId": "02fcc324-435e-4a0a-a79d-f1ee60ed5033",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab size 7919\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import *\n",
        "from keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.models import Model\n",
        "from keras import regularizers\n",
        "sequence_length = data.shape[1]\n",
        "filter_sizes = [3,4,5]\n",
        "num_filters = 100\n",
        "drop = 0.5\n",
        "\n",
        "inputs = Input(shape=(sequence_length,))\n",
        "embedding = embedding_layer(inputs)\n",
        "\n",
        "################## LSTM ONLY ###############################\n",
        "# reshape = Reshape((sequence_length,EMBEDDING_DIM))(embedding)\n",
        "\n",
        "################# SINGLE LSTM ####################\n",
        "# lstm_0 = LSTM(512)(reshape)\n",
        "\n",
        "# YOU WANNA ADD MORE LSTM LAYERS? UNCOMMENT THIS #\n",
        "# lstm_2 = LSTM(1024, return_sequences=True)(reshape)\n",
        "# lstm_1 = LSTM(512, return_sequences=True)(lstm_2)\n",
        "# lstm_0 = LSTM(256)(lstm_1)\n",
        "\n",
        "############################################################\n",
        "\n",
        "\n",
        "################## CRNN ####################################\n",
        "reshape = Reshape((sequence_length,EMBEDDING_DIM))(embedding)\n",
        "\n",
        "conv_0 = Conv1D(num_filters, (filter_sizes[0], ),padding=\"same\",activation='relu',kernel_regularizer=regularizers.l2(0.01))(reshape)\n",
        "conv_1 = Conv1D(num_filters, (filter_sizes[1], ),padding=\"same\",activation='relu',kernel_regularizer=regularizers.l2(0.01))(reshape)\n",
        "conv_2 = Conv1D(num_filters, (filter_sizes[2], ),padding=\"same\",activation='relu',kernel_regularizer=regularizers.l2(0.01))(reshape)\n",
        "\n",
        "conv_0 = MaxPool1D(300)(conv_0)\n",
        "conv_1 = MaxPool1D(300)(conv_1)\n",
        "conv_2 = MaxPool1D(300)(conv_2)\n",
        "# Reshape output to match RNN dimension\n",
        "# conv_0 = Reshape((-1, num_filters))(conv_0)\n",
        "# conv_1 = Reshape((-1, num_filters))(conv_1)\n",
        "# conv_2 = Reshape((-1, num_filters))(conv_2)\n",
        "\n",
        "concat = concatenate([conv_0, conv_1, conv_2])\n",
        "concat = Flatten()(concat)\n",
        "# lstm_0 = LSTM(512)(concat)\n",
        "\n",
        "# YOU WANNA ADD MORE LSTM LAYERS? UNCOMMENT THIS #\n",
        "# lstm_2 = LSTM(1024, return_sequences=True)(concat)\n",
        "# lstm_1 = LSTM(512, return_sequences=True)(lstm_2)\n",
        "# lstm_0 = LSTM(256)(lstm_1)\n",
        "\n",
        "############################################################\n",
        "\n",
        "dropout = Dropout(drop)(concat)\n",
        "output = Dense(units=3, activation='softmax',kernel_regularizer=regularizers.l2(0.01))(dropout)\n",
        "\n",
        "# this creates a model that includes\n",
        "model = Model(inputs, output)\n",
        "\n",
        "\n",
        "adam = Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kMeQGJ5JqlnJ",
        "outputId": "9dd49473-12cd-40b9-d956-6dcd48994904"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)        [(None, 300)]                0         []                            \n",
            "                                                                                                  \n",
            " embedding (Embedding)       (None, 300, 400)             3167600   ['input_3[0][0]']             \n",
            "                                                                                                  \n",
            " reshape_2 (Reshape)         (None, 300, 400)             0         ['embedding[2][0]']           \n",
            "                                                                                                  \n",
            " conv1d_6 (Conv1D)           (None, 300, 100)             120100    ['reshape_2[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_7 (Conv1D)           (None, 300, 100)             160100    ['reshape_2[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_8 (Conv1D)           (None, 300, 100)             200100    ['reshape_2[0][0]']           \n",
            "                                                                                                  \n",
            " max_pooling1d_6 (MaxPoolin  (None, 1, 100)               0         ['conv1d_6[0][0]']            \n",
            " g1D)                                                                                             \n",
            "                                                                                                  \n",
            " max_pooling1d_7 (MaxPoolin  (None, 1, 100)               0         ['conv1d_7[0][0]']            \n",
            " g1D)                                                                                             \n",
            "                                                                                                  \n",
            " max_pooling1d_8 (MaxPoolin  (None, 1, 100)               0         ['conv1d_8[0][0]']            \n",
            " g1D)                                                                                             \n",
            "                                                                                                  \n",
            " concatenate_2 (Concatenate  (None, 1, 300)               0         ['max_pooling1d_6[0][0]',     \n",
            " )                                                                   'max_pooling1d_7[0][0]',     \n",
            "                                                                     'max_pooling1d_8[0][0]']     \n",
            "                                                                                                  \n",
            " flatten_2 (Flatten)         (None, 300)                  0         ['concatenate_2[0][0]']       \n",
            "                                                                                                  \n",
            " dropout_2 (Dropout)         (None, 300)                  0         ['flatten_2[0][0]']           \n",
            "                                                                                                  \n",
            " dense_2 (Dense)             (None, 3)                    903       ['dropout_2[0][0]']           \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 3648803 (13.92 MB)\n",
            "Trainable params: 3648803 (13.92 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#define callbacks\n",
        "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=4, verbose=1)\n",
        "callbacks_list = [early_stopping]\n",
        "\n",
        "model.fit(data, labels, validation_split=0.2,\n",
        "          epochs=10, batch_size=256, callbacks=callbacks_list, shuffle=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0DszSi3rECn",
        "outputId": "1d706253-e8c9-4356-fae5-1ba8d95e35b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "16/16 [==============================] - 126s 8s/step - loss: 7.2972 - accuracy: 0.4588 - val_loss: 7.2478 - val_accuracy: 0.0824\n",
            "Epoch 2/10\n",
            "16/16 [==============================] - 105s 7s/step - loss: 5.4355 - accuracy: 0.6235 - val_loss: 5.5564 - val_accuracy: 0.3078\n",
            "Epoch 3/10\n",
            "16/16 [==============================] - 103s 6s/step - loss: 4.5501 - accuracy: 0.6990 - val_loss: 6.4581 - val_accuracy: 0.0529\n",
            "Epoch 4/10\n",
            "16/16 [==============================] - 106s 7s/step - loss: 3.8344 - accuracy: 0.7919 - val_loss: 5.3081 - val_accuracy: 0.1333\n",
            "Epoch 5/10\n",
            "16/16 [==============================] - 106s 7s/step - loss: 3.3220 - accuracy: 0.8328 - val_loss: 4.8043 - val_accuracy: 0.1608\n",
            "Epoch 6/10\n",
            "16/16 [==============================] - 108s 7s/step - loss: 2.9084 - accuracy: 0.8760 - val_loss: 4.8475 - val_accuracy: 0.0843\n",
            "Epoch 7/10\n",
            "16/16 [==============================] - 111s 7s/step - loss: 2.5620 - accuracy: 0.8902 - val_loss: 4.0822 - val_accuracy: 0.1853\n",
            "Epoch 8/10\n",
            "16/16 [==============================] - 104s 7s/step - loss: 2.2393 - accuracy: 0.9203 - val_loss: 4.4211 - val_accuracy: 0.0833\n",
            "Epoch 9/10\n",
            "16/16 [==============================] - 105s 7s/step - loss: 1.9893 - accuracy: 0.9255 - val_loss: 3.8701 - val_accuracy: 0.1235\n",
            "Epoch 10/10\n",
            "16/16 [==============================] - 104s 7s/step - loss: 1.7620 - accuracy: 0.9350 - val_loss: 3.7544 - val_accuracy: 0.1157\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7cdf5e085420>"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels_test = data_test.iloc[:, 0].values\n",
        "reviews_test = data_test.iloc[:, 1].values"
      ],
      "metadata": {
        "id": "Fqt7BqyQ02LU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_labels_test = []\n",
        "\n",
        "for label_test in labels_test:\n",
        "    if label_test == -1:\n",
        "        encoded_labels_test.append([1,0,0])\n",
        "    elif label_test == 0:\n",
        "        encoded_labels_test.append([0,1,0])\n",
        "    else:\n",
        "        encoded_labels_test.append([0,0,1])\n",
        "\n",
        "encoded_labels_test = np.array(encoded_labels_test)"
      ],
      "metadata": {
        "id": "AYMB_iY307KD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reviews_processed_test = []\n",
        "unlabeled_processed_test = []\n",
        "for review_test in reviews_test:\n",
        "    review_cool_one = ''.join([char for char in review_test if char not in digits])\n",
        "    reviews_processed_test.append(review_cool_one)"
      ],
      "metadata": {
        "id": "t5OH84-C09Xm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Use PyVi for Vietnamese word tokenizer\n",
        "word_reviews_test = []\n",
        "all_words = []\n",
        "for review_test in reviews_processed_test:\n",
        "    review_test = ViTokenizer.tokenize(review_test.lower())\n",
        "    word_reviews_test.append(review_test.split())"
      ],
      "metadata": {
        "id": "qdtJQOrp0-_c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences_test = tokenizer.texts_to_sequences(word_reviews_test)\n",
        "data_test = pad_sequences(sequences_test, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "labels_test = encoded_labels_test"
      ],
      "metadata": {
        "id": "L5HOe_LM1AUG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Shape of X train and X validation tensor:',data_test.shape)\n",
        "print('Shape of label train and validation tensor:', labels_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HywiRROm1Br5",
        "outputId": "887bd145-751f-4023-8d02-ac26e6d1828e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X train and X validation tensor: (1050, 300)\n",
            "Shape of label train and validation tensor: (1050, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "score = model.evaluate(data_test, labels_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X5D2Nc-11DM9",
        "outputId": "85fa78d5-d1dc-4b45-9703-8ec7ac895a0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "33/33 [==============================] - 6s 184ms/step - loss: 2.4005 - accuracy: 0.5981\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"%s: %.2f\" % (model.metrics_names[0], score[0]))\n",
        "print(\"%s: %.2f%%\" % (model.metrics_names[1], score[1]*100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GHzBp64H1Ijj",
        "outputId": "25040c3b-ddf3-458d-c374-cabaad5ee6f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 2.40\n",
            "accuracy: 59.81%\n"
          ]
        }
      ]
    }
  ]
}