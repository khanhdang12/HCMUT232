{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "t-fVpdguvl8J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1a0d4ea-6bae-41f1-92c4-2a070dec6a3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyvi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Illny3xTNTnO",
        "outputId": "d3a89fd5-10f3-455a-8f16-3d53bdafe173"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyvi\n",
            "  Downloading pyvi-0.1.1-py2.py3-none-any.whl (8.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from pyvi) (1.2.2)\n",
            "Collecting sklearn-crfsuite (from pyvi)\n",
            "  Downloading sklearn_crfsuite-0.3.6-py2.py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->pyvi) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->pyvi) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->pyvi) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->pyvi) (3.5.0)\n",
            "Collecting python-crfsuite>=0.8.3 (from sklearn-crfsuite->pyvi)\n",
            "  Downloading python_crfsuite-0.9.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from sklearn-crfsuite->pyvi) (1.16.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from sklearn-crfsuite->pyvi) (0.9.0)\n",
            "Requirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.10/dist-packages (from sklearn-crfsuite->pyvi) (4.66.4)\n",
            "Installing collected packages: python-crfsuite, sklearn-crfsuite, pyvi\n",
            "Successfully installed python-crfsuite-0.9.10 pyvi-0.1.1 sklearn-crfsuite-0.3.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from string import digits\n",
        "from collections import Counter\n",
        "from pyvi import ViTokenizer\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "yCwD52tXNX0A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_train = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/NLP/Lab5/vlsp_sentiment_train.csv\", sep='\\t')\n",
        "data_train.columns =['Class', 'Data']\n",
        "data_test = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/NLP/Lab5/vlsp_sentiment_test.csv\", sep='\\t')\n",
        "data_test.columns =['Class', 'Data']"
      ],
      "metadata": {
        "id": "Nq7zj9O5Nc0U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data_train.shape)\n",
        "print(data_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cubSqEN1Ne_r",
        "outputId": "83414b9e-fc85-413d-900a-3904850545f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5100, 2)\n",
            "(1050, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels = data_train.iloc[:, 0].values\n",
        "reviews = data_train.iloc[:, 1].values"
      ],
      "metadata": {
        "id": "xOS31S3rNgmh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_labels = []\n",
        "\n",
        "for label in labels:\n",
        "    if label == -1:\n",
        "        encoded_labels.append([1,0,0])\n",
        "    elif label == 0:\n",
        "        encoded_labels.append([0,1,0])\n",
        "    else:\n",
        "        encoded_labels.append([0,0,1])\n",
        "\n",
        "encoded_labels = np.array(encoded_labels)"
      ],
      "metadata": {
        "id": "e85tzLU-Nh-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reviews_processed = []\n",
        "unlabeled_processed = []\n",
        "for review in reviews:\n",
        "    review_cool_one = ''.join([char for char in review if char not in digits])\n",
        "    reviews_processed.append(review_cool_one)"
      ],
      "metadata": {
        "id": "HRY8-r5bNkW6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Use PyVi for Vietnamese word tokenizer\n",
        "word_reviews = []\n",
        "all_words = []\n",
        "for review in reviews_processed:\n",
        "    review = ViTokenizer.tokenize(review.lower())\n",
        "    word_reviews.append(review.split())"
      ],
      "metadata": {
        "id": "pDTtvz5UNmix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EMBEDDING_DIM = 400 # how big is each word vector\n",
        "MAX_VOCAB_SIZE = 10000 # how many unique words to use (i.e num rows in embedding vector)\n",
        "MAX_SEQUENCE_LENGTH = 300 # max number of words in a comment to use"
      ],
      "metadata": {
        "id": "RKtzkN1FNoWH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "metadata": {
        "id": "HS167O7mNojJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE, lower=True, char_level=False)\n",
        "tokenizer.fit_on_texts(word_reviews)\n",
        "sequences_train = tokenizer.texts_to_sequences(word_reviews)\n",
        "word_index = tokenizer.word_index"
      ],
      "metadata": {
        "id": "vWxuIE7sNtKR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pad_sequences(sequences_train, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "labels = encoded_labels"
      ],
      "metadata": {
        "id": "RMFXJWIyNu92"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Shape of X train and X validation tensor:',data.shape)\n",
        "print('Shape of label train and validation tensor:', labels.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bHxPoBTCNwZQ",
        "outputId": "48fdab59-8987-47f9-951e-5229d4b36c34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X train and X validation tensor: (5100, 300)\n",
            "Shape of label train and validation tensor: (5100, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.utils import simple_preprocess\n",
        "\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "\n",
        "word_vectors = KeyedVectors.load_word2vec_format('/content/drive/MyDrive/Colab Notebooks/NLP/Lab5/vi-model-CBOW.bin', binary=True)\n",
        "\n",
        "\n",
        "vocabulary_size=min(len(word_index)+1,MAX_VOCAB_SIZE)\n",
        "embedding_matrix = np.zeros((vocabulary_size, EMBEDDING_DIM))\n",
        "print(\"vocab size\", vocabulary_size)\n",
        "for word, i in word_index.items():\n",
        "    if i>=MAX_VOCAB_SIZE:\n",
        "        continue\n",
        "    try:\n",
        "        embedding_vector = word_vectors[word]\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "    except KeyError:\n",
        "        embedding_matrix[i]=np.random.normal(0,np.sqrt(0.25),EMBEDDING_DIM)\n",
        "\n",
        "del(word_vectors)\n",
        "\n",
        "from keras.layers import Embedding\n",
        "embedding_layer = Embedding(vocabulary_size,\n",
        "                            EMBEDDING_DIM,\n",
        "                            weights=[embedding_matrix],\n",
        "                            trainable=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dUmf5cSCNx_F",
        "outputId": "e516852e-e5d9-43c8-fb7a-bf674209a540"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab size 7919\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Dense, Input, Embedding, Dropout, Conv1D, MaxPooling1D, LSTM, concatenate, Flatten, Reshape\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import regularizers\n",
        "\n",
        "sequence_length = data.shape[1]\n",
        "filter_sizes = [3, 4, 5]\n",
        "num_filters = 100\n",
        "drop = 0.5\n",
        "\n",
        "# Input layer\n",
        "inputs = Input(shape=(sequence_length,))\n",
        "\n",
        "# Embedding layer\n",
        "embedding = embedding_layer(inputs)\n",
        "\n",
        "# CNN layers\n",
        "conv_0 = Conv1D(num_filters, filter_sizes[0], activation='relu', kernel_regularizer=regularizers.l2(0.01))(embedding)\n",
        "conv_1 = Conv1D(num_filters, filter_sizes[1], activation='relu', kernel_regularizer=regularizers.l2(0.01))(embedding)\n",
        "conv_2 = Conv1D(num_filters, filter_sizes[2], activation='relu', kernel_regularizer=regularizers.l2(0.01))(embedding)\n",
        "\n",
        "# Max pooling layer\n",
        "maxpool_0 = MaxPooling1D(sequence_length - filter_sizes[0] + 1, strides=1)(conv_0)\n",
        "maxpool_1 = MaxPooling1D(sequence_length - filter_sizes[1] + 1, strides=1)(conv_1)\n",
        "maxpool_2 = MaxPooling1D(sequence_length - filter_sizes[2] + 1, strides=1)(conv_2)\n",
        "\n",
        "# Concatenate pooled features\n",
        "merged_tensor = concatenate([maxpool_0, maxpool_1, maxpool_2], axis=1)\n",
        "flatten = Flatten()(merged_tensor)\n",
        "\n",
        "# LSTM layer\n",
        "# reshaped_tensor = Reshape((num_filters * len(filter_sizes),))(flatten)\n",
        "# lstm = LSTM(128)(reshaped_tensor)\n",
        "reshaped_tensor = Reshape((1, num_filters * len(filter_sizes)))(flatten)\n",
        "lstm = LSTM(128)(reshaped_tensor)\n",
        "\n",
        "# Dropout layer\n",
        "dropout = Dropout(drop)(lstm)\n",
        "\n",
        "# Output layer\n",
        "output = Dense(units=3, activation='softmax', kernel_regularizer=regularizers.l2(0.01))(dropout)\n",
        "\n",
        "# Define the model\n",
        "model = Model(inputs, output)\n",
        "\n",
        "# Compile the model\n",
        "adam = Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
        "\n",
        "# Model summary\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "HiKMOV3QvjMG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e9741ca-59d6-4f22-ec2a-c7a5bc896f1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_7 (InputLayer)        [(None, 300)]                0         []                            \n",
            "                                                                                                  \n",
            " embedding (Embedding)       (None, 300, 400)             3167600   ['input_7[0][0]']             \n",
            "                                                                                                  \n",
            " conv1d_18 (Conv1D)          (None, 298, 100)             120100    ['embedding[6][0]']           \n",
            "                                                                                                  \n",
            " conv1d_19 (Conv1D)          (None, 297, 100)             160100    ['embedding[6][0]']           \n",
            "                                                                                                  \n",
            " conv1d_20 (Conv1D)          (None, 296, 100)             200100    ['embedding[6][0]']           \n",
            "                                                                                                  \n",
            " max_pooling1d_18 (MaxPooli  (None, 1, 100)               0         ['conv1d_18[0][0]']           \n",
            " ng1D)                                                                                            \n",
            "                                                                                                  \n",
            " max_pooling1d_19 (MaxPooli  (None, 1, 100)               0         ['conv1d_19[0][0]']           \n",
            " ng1D)                                                                                            \n",
            "                                                                                                  \n",
            " max_pooling1d_20 (MaxPooli  (None, 1, 100)               0         ['conv1d_20[0][0]']           \n",
            " ng1D)                                                                                            \n",
            "                                                                                                  \n",
            " concatenate_6 (Concatenate  (None, 3, 100)               0         ['max_pooling1d_18[0][0]',    \n",
            " )                                                                   'max_pooling1d_19[0][0]',    \n",
            "                                                                     'max_pooling1d_20[0][0]']    \n",
            "                                                                                                  \n",
            " flatten_6 (Flatten)         (None, 300)                  0         ['concatenate_6[0][0]']       \n",
            "                                                                                                  \n",
            " reshape_3 (Reshape)         (None, 1, 300)               0         ['flatten_6[0][0]']           \n",
            "                                                                                                  \n",
            " lstm_3 (LSTM)               (None, 128)                  219648    ['reshape_3[0][0]']           \n",
            "                                                                                                  \n",
            " dropout (Dropout)           (None, 128)                  0         ['lstm_3[0][0]']              \n",
            "                                                                                                  \n",
            " dense (Dense)               (None, 3)                    387       ['dropout[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 3867935 (14.76 MB)\n",
            "Trainable params: 3867935 (14.76 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#define callbacks\n",
        "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=4, verbose=1)\n",
        "callbacks_list = [early_stopping]\n",
        "\n",
        "model.fit(data, labels, validation_split=0.2,\n",
        "          epochs=5, batch_size=256, callbacks=callbacks_list, shuffle=True)"
      ],
      "metadata": {
        "id": "_QTlV5zmuXmZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc31d928-fe89-4d26-e3af-189c546ef4bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "16/16 [==============================] - 104s 6s/step - loss: 4.8533 - accuracy: 0.4679 - val_loss: 4.5394 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/5\n",
            "16/16 [==============================] - 98s 6s/step - loss: 2.7955 - accuracy: 0.6037 - val_loss: 2.8256 - val_accuracy: 0.0000e+00\n",
            "Epoch 3/5\n",
            "16/16 [==============================] - 98s 6s/step - loss: 1.6658 - accuracy: 0.6792 - val_loss: 2.3028 - val_accuracy: 0.0186\n",
            "Epoch 4/5\n",
            "16/16 [==============================] - 95s 6s/step - loss: 1.1416 - accuracy: 0.7608 - val_loss: 1.9117 - val_accuracy: 0.1520\n",
            "Epoch 5/5\n",
            "16/16 [==============================] - 94s 6s/step - loss: 0.8482 - accuracy: 0.8591 - val_loss: 2.0750 - val_accuracy: 0.1275\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x78e707b33910>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels_test = data_test.iloc[:, 0].values\n",
        "reviews_test = data_test.iloc[:, 1].values"
      ],
      "metadata": {
        "id": "3Z4iq3OhZ-Sd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_labels_test = []\n",
        "\n",
        "for label_test in labels_test:\n",
        "    if label_test == -1:\n",
        "        encoded_labels_test.append([1,0,0])\n",
        "    elif label_test == 0:\n",
        "        encoded_labels_test.append([0,1,0])\n",
        "    else:\n",
        "        encoded_labels_test.append([0,0,1])\n",
        "\n",
        "encoded_labels_test = np.array(encoded_labels_test)"
      ],
      "metadata": {
        "id": "5KoCW9onZ_3i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reviews_processed_test = []\n",
        "# unlabeled_processed_test = []\n",
        "# for review_test in reviews_test:\n",
        "#     review_cool_one = ''.join([char for char in review_test if char not in digits])\n",
        "#     reviews_processed_test.append(review_cool_one)\n",
        "reviews_processed_test = []\n",
        "unlabeled_processed_test = []\n",
        "for review_test in reviews_test:\n",
        "    review_test = str(review_test)  # Convert to string if not already\n",
        "    review_cool_one = ''.join([char for char in review_test if char not in digits])\n",
        "    reviews_processed_test.append(review_cool_one)"
      ],
      "metadata": {
        "id": "MjZxiBUYaE3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Use PyVi for Vietnamese word tokenizer\n",
        "word_reviews_test = []\n",
        "all_words = []\n",
        "for review_test in reviews_processed_test:\n",
        "    review_test = ViTokenizer.tokenize(review_test.lower())\n",
        "    word_reviews_test.append(review_test.split())"
      ],
      "metadata": {
        "id": "VOsYbjfhaGpZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences_test = tokenizer.texts_to_sequences(word_reviews_test)\n",
        "data_test = pad_sequences(sequences_test, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "labels_test = encoded_labels_test"
      ],
      "metadata": {
        "id": "7Q-MInu-aILW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Shape of X train and X validation tensor:',data_test.shape)\n",
        "print('Shape of label train and validation tensor:', labels_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XN0ZEG7_aJpH",
        "outputId": "0524c708-8da7-46a9-c14a-e2be142a1621"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X train and X validation tensor: (1050, 300)\n",
            "Shape of label train and validation tensor: (1050, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "score = model.evaluate(data_test, labels_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7fFGLSbsaLQw",
        "outputId": "ea1fe2ac-8b64-4ca4-ccb5-2a4cca0bb180"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "33/33 [==============================] - 7s 223ms/step - loss: 1.2536 - accuracy: 0.6067\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"%s: %.2f\" % (model.metrics_names[0], score[0]))\n",
        "print(\"%s: %.2f%%\" % (model.metrics_names[1], score[1]*100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5J73lwZkaOQ-",
        "outputId": "fd82c515-3c23-401f-c405-a933946f44ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 1.25\n",
            "accuracy: 60.67%\n"
          ]
        }
      ]
    }
  ]
}