{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C43miXz8bn_5",
        "outputId": "7500981a-9c17-4cbd-8a7f-47075e098187"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyvi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xQWRmvL7b0wU",
        "outputId": "3ed50251-87f0-4dc7-a247-6b1161e46183"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyvi\n",
            "  Downloading pyvi-0.1.1-py2.py3-none-any.whl (8.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m46.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from pyvi) (1.2.2)\n",
            "Collecting sklearn-crfsuite (from pyvi)\n",
            "  Downloading sklearn_crfsuite-0.3.6-py2.py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->pyvi) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->pyvi) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->pyvi) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->pyvi) (3.5.0)\n",
            "Collecting python-crfsuite>=0.8.3 (from sklearn-crfsuite->pyvi)\n",
            "  Downloading python_crfsuite-0.9.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m55.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from sklearn-crfsuite->pyvi) (1.16.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from sklearn-crfsuite->pyvi) (0.9.0)\n",
            "Requirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.10/dist-packages (from sklearn-crfsuite->pyvi) (4.66.4)\n",
            "Installing collected packages: python-crfsuite, sklearn-crfsuite, pyvi\n",
            "Successfully installed python-crfsuite-0.9.10 pyvi-0.1.1 sklearn-crfsuite-0.3.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from string import digits\n",
        "from collections import Counter\n",
        "from pyvi import ViTokenizer\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "q8cG6dvGb5V6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_train = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/NLP/Lab5/vlsp_sentiment_train.csv\", sep='\\t')\n",
        "data_train.columns =['Class', 'Data']\n",
        "data_test = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/NLP/Lab5/vlsp_sentiment_test.csv\", sep='\\t')\n",
        "data_test.columns =['Class', 'Data']"
      ],
      "metadata": {
        "id": "rrr_uFbScb1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data_train.shape)\n",
        "print(data_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84TNVEqncoGX",
        "outputId": "80978d8c-e10a-4fd5-b29d-443059738d78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5100, 2)\n",
            "(1050, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels = data_train.iloc[:, 0].values\n",
        "reviews = data_train.iloc[:, 1].values"
      ],
      "metadata": {
        "id": "fiQbytv9cqD_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_labels = []\n",
        "\n",
        "for label in labels:\n",
        "    if label == -1:\n",
        "        encoded_labels.append([1,0,0])\n",
        "    elif label == 0:\n",
        "        encoded_labels.append([0,1,0])\n",
        "    else:\n",
        "        encoded_labels.append([0,0,1])\n",
        "\n",
        "encoded_labels = np.array(encoded_labels)"
      ],
      "metadata": {
        "id": "gKMxwn_Ycrff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reviews_processed = []\n",
        "unlabeled_processed = []\n",
        "for review in reviews:\n",
        "    review_cool_one = ''.join([char for char in review if char not in digits])\n",
        "    reviews_processed.append(review_cool_one)"
      ],
      "metadata": {
        "id": "mYDlTSfkctYS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Use PyVi for Vietnamese word tokenizer\n",
        "word_reviews = []\n",
        "all_words = []\n",
        "for review in reviews_processed:\n",
        "    review = ViTokenizer.tokenize(review.lower())\n",
        "    word_reviews.append(review.split())"
      ],
      "metadata": {
        "id": "_U8-yS3_cvFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EMBEDDING_DIM = 400 # how big is each word vector\n",
        "MAX_VOCAB_SIZE = 10000 # how many unique words to use (i.e num rows in embedding vector)\n",
        "MAX_SEQUENCE_LENGTH = 300 # max number of words in a comment to use"
      ],
      "metadata": {
        "id": "a9FXkh9pcx-X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "metadata": {
        "id": "2PB41blEczbD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE, lower=True, char_level=False)\n",
        "tokenizer.fit_on_texts(word_reviews)\n",
        "sequences_train = tokenizer.texts_to_sequences(word_reviews)\n",
        "word_index = tokenizer.word_index"
      ],
      "metadata": {
        "id": "UzAgRRobc1Hn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pad_sequences(sequences_train, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "labels = encoded_labels"
      ],
      "metadata": {
        "id": "M6kxwSXuc2Xv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Shape of X train and X validation tensor:',data.shape)\n",
        "print('Shape of label train and validation tensor:', labels.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Al54vtBc4FC",
        "outputId": "e2d90b19-f7bd-44b7-fe14-0c3bb64cf1be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X train and X validation tensor: (5100, 300)\n",
            "Shape of label train and validation tensor: (5100, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.utils import simple_preprocess\n",
        "\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "\n",
        "word_vectors = KeyedVectors.load_word2vec_format('/content/drive/MyDrive/Colab Notebooks/NLP/Lab5/vi-model-CBOW.bin', binary=True)\n",
        "\n",
        "\n",
        "vocabulary_size=min(len(word_index)+1,MAX_VOCAB_SIZE)\n",
        "embedding_matrix = np.zeros((vocabulary_size, EMBEDDING_DIM))\n",
        "print(\"vocab size\", vocabulary_size)\n",
        "for word, i in word_index.items():\n",
        "    if i>=MAX_VOCAB_SIZE:\n",
        "        continue\n",
        "    try:\n",
        "        embedding_vector = word_vectors[word]\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "    except KeyError:\n",
        "        embedding_matrix[i]=np.random.normal(0,np.sqrt(0.25),EMBEDDING_DIM)\n",
        "\n",
        "del(word_vectors)\n",
        "\n",
        "from keras.layers import Embedding\n",
        "embedding_layer = Embedding(vocabulary_size,\n",
        "                            EMBEDDING_DIM,\n",
        "                            weights=[embedding_matrix],\n",
        "                            trainable=True)"
      ],
      "metadata": {
        "id": "_jURCZNSc9LP",
        "outputId": "64f54e4f-6247-42e5-d2eb-3c806c9ed73d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab size 7919\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Dense, Input, GlobalMaxPooling1D\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Embedding\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Embedding, Dropout,concatenate\n",
        "from tensorflow.keras.layers import Reshape, Flatten\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import regularizers\n",
        "sequence_length = data.shape[1]\n",
        "filter_sizes = [3,4,5]\n",
        "num_filters = 100\n",
        "drop = 0.5\n",
        "\n",
        "inputs = Input(shape=(sequence_length,))\n",
        "embedding = embedding_layer(inputs)\n",
        "# reshape = Reshape((sequence_length,EMBEDDING_DIM,1))(embedding)\n",
        "\n",
        "conv_0 = Conv1D(num_filters, filter_sizes[0],activation='relu',kernel_regularizer=regularizers.l2(0.01))(embedding)\n",
        "conv_1 = Conv1D(num_filters, filter_sizes[1],activation='relu',kernel_regularizer=regularizers.l2(0.01))(embedding)\n",
        "conv_2 = Conv1D(num_filters, filter_sizes[2],activation='relu',kernel_regularizer=regularizers.l2(0.01))(embedding)\n",
        "print(conv_1)\n",
        "maxpool_0 = MaxPooling1D(sequence_length - filter_sizes[0] + 1, strides=1)(conv_0)\n",
        "maxpool_1 = MaxPooling1D(sequence_length - filter_sizes[1] + 1, strides=1)(conv_1)\n",
        "maxpool_2 = MaxPooling1D(sequence_length - filter_sizes[2] + 1, strides=1)(conv_2)\n",
        "\n",
        "merged_tensor = concatenate([maxpool_0, maxpool_1, maxpool_2], axis=1)\n",
        "flatten = Flatten()(merged_tensor)\n",
        "reshape = Reshape((3*num_filters,))(flatten)\n",
        "dropout = Dropout(drop)(flatten)\n",
        "output = Dense(units=3, activation='softmax',kernel_regularizer=regularizers.l2(0.01))(dropout)\n",
        "\n",
        "\n",
        "# this creates a model that includes\n",
        "model = Model(inputs, output)\n",
        "\n",
        "\n",
        "# adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
        "adam = Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "#define callbacks\n",
        "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=4, verbose=1)\n",
        "callbacks_list = [early_stopping]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CwetjP8ldJv1",
        "outputId": "db3d75e2-cc6a-43c7-e9f9-4cb218b52e8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KerasTensor(type_spec=TensorSpec(shape=(None, 297, 100), dtype=tf.float32, name=None), name='conv1d_1/Relu:0', description=\"created by layer 'conv1d_1'\")\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)        [(None, 300)]                0         []                            \n",
            "                                                                                                  \n",
            " embedding (Embedding)       (None, 300, 400)             3167600   ['input_1[0][0]']             \n",
            "                                                                                                  \n",
            " conv1d (Conv1D)             (None, 298, 100)             120100    ['embedding[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_1 (Conv1D)           (None, 297, 100)             160100    ['embedding[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_2 (Conv1D)           (None, 296, 100)             200100    ['embedding[0][0]']           \n",
            "                                                                                                  \n",
            " max_pooling1d (MaxPooling1  (None, 1, 100)               0         ['conv1d[0][0]']              \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " max_pooling1d_1 (MaxPoolin  (None, 1, 100)               0         ['conv1d_1[0][0]']            \n",
            " g1D)                                                                                             \n",
            "                                                                                                  \n",
            " max_pooling1d_2 (MaxPoolin  (None, 1, 100)               0         ['conv1d_2[0][0]']            \n",
            " g1D)                                                                                             \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)   (None, 3, 100)               0         ['max_pooling1d[0][0]',       \n",
            "                                                                     'max_pooling1d_1[0][0]',     \n",
            "                                                                     'max_pooling1d_2[0][0]']     \n",
            "                                                                                                  \n",
            " flatten (Flatten)           (None, 300)                  0         ['concatenate[0][0]']         \n",
            "                                                                                                  \n",
            " dropout (Dropout)           (None, 300)                  0         ['flatten[0][0]']             \n",
            "                                                                                                  \n",
            " dense (Dense)               (None, 3)                    903       ['dropout[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 3648803 (13.92 MB)\n",
            "Trainable params: 3648803 (13.92 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(data, labels, validation_split=0.2,\n",
        "          epochs=5, batch_size=256, callbacks=callbacks_list, shuffle=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1T4-luqidff6",
        "outputId": "a8890a8a-1937-47ad-ef46-7338a0850fd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "16/16 [==============================] - 97s 6s/step - loss: 7.7481 - accuracy: 0.4297 - val_loss: 8.2957 - val_accuracy: 0.0186\n",
            "Epoch 2/5\n",
            "16/16 [==============================] - 93s 6s/step - loss: 5.6524 - accuracy: 0.6135 - val_loss: 7.5700 - val_accuracy: 0.0392\n",
            "Epoch 3/5\n",
            "16/16 [==============================] - 93s 6s/step - loss: 4.7240 - accuracy: 0.6941 - val_loss: 6.9440 - val_accuracy: 0.0412\n",
            "Epoch 4/5\n",
            "16/16 [==============================] - 93s 6s/step - loss: 4.1104 - accuracy: 0.7588 - val_loss: 6.2269 - val_accuracy: 0.0500\n",
            "Epoch 5/5\n",
            "16/16 [==============================] - 93s 6s/step - loss: 3.5650 - accuracy: 0.8199 - val_loss: 5.0810 - val_accuracy: 0.1225\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x798e71ba4430>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels_test = data_test.iloc[:, 0].values\n",
        "reviews_test = data_test.iloc[:, 1].values"
      ],
      "metadata": {
        "id": "u_VdfyfmhY3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_labels_test = []\n",
        "\n",
        "for label_test in labels_test:\n",
        "    if label_test == -1:\n",
        "        encoded_labels_test.append([1,0,0])\n",
        "    elif label_test == 0:\n",
        "        encoded_labels_test.append([0,1,0])\n",
        "    else:\n",
        "        encoded_labels_test.append([0,0,1])\n",
        "\n",
        "encoded_labels_test = np.array(encoded_labels_test)"
      ],
      "metadata": {
        "id": "aDAi6XuJhabF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reviews_processed_test = []\n",
        "unlabeled_processed_test = []\n",
        "for review_test in reviews_test:\n",
        "    review_cool_one = ''.join([char for char in review_test if char not in digits])\n",
        "    reviews_processed_test.append(review_cool_one)"
      ],
      "metadata": {
        "id": "DJsx9kQ_hb8s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Use PyVi for Vietnamese word tokenizer\n",
        "word_reviews_test = []\n",
        "all_words = []\n",
        "for review_test in reviews_processed_test:\n",
        "    review_test = ViTokenizer.tokenize(review_test.lower())\n",
        "    word_reviews_test.append(review_test.split())"
      ],
      "metadata": {
        "id": "39qwNBg7hdeT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences_test = tokenizer.texts_to_sequences(word_reviews_test)\n",
        "data_test = pad_sequences(sequences_test, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "labels_test = encoded_labels_test"
      ],
      "metadata": {
        "id": "XYLmrBuWheZ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Shape of X train and X validation tensor:',data_test.shape)\n",
        "print('Shape of label train and validation tensor:', labels_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MlwUiJ8jhgQR",
        "outputId": "8b915d4f-fbe6-4ca3-d58d-9b87b383395c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X train and X validation tensor: (1050, 300)\n",
            "Shape of label train and validation tensor: (1050, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "score = model.evaluate(data_test, labels_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxefNOVehh2N",
        "outputId": "1d1a68c9-4a07-482a-f46f-a4b7f15114ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "33/33 [==============================] - 8s 232ms/step - loss: 3.8896 - accuracy: 0.5990\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"%s: %.2f\" % (model.metrics_names[0], score[0]))\n",
        "print(\"%s: %.2f%%\" % (model.metrics_names[1], score[1]*100))"
      ],
      "metadata": {
        "id": "-1M-26jYhnp6",
        "outputId": "24897c2d-a44a-4ca3-8a20-8519b301ae24",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 3.89\n",
            "accuracy: 59.90%\n"
          ]
        }
      ]
    }
  ]
}